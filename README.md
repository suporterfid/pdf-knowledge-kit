# PDF ‚Üí Vector DB (pgvector) Starter Kit

Crie rapidamente uma base de conhecimento a partir de **arquivos PDF** em uma pasta local e habilite **busca sem√¢ntica** para um agente de IA.

## Vis√£o geral
1. **Extrai** textos dos PDFs.
2. **Divide** em *chunks* (trechos) com sobreposi√ß√£o.
3. **Gera embeddings** (multil√≠ngue, PT/EN) com `fastembed`.
4. **Armazena** em **PostgreSQL + pgvector**.
5. **Consulta** por similaridade (kNN) com `query.py` ‚Äî pronto para integrar no seu agente.

> Dimens√£o dos vetores: **384** (modelo `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`).

---

## Requisitos
- **Docker** + **Docker Compose** (para o Postgres com pgvector).
- **Python 3.10+** com `pip`.

## Passo a passo (r√°pido)
```bash
# 1) Suba o Postgres com pgvector
docker compose up -d db

# 2) Instale as depend√™ncias Python
python -m venv .venv && source .venv/bin/activate  # no Windows: .venv\Scripts\activate
pip install -r requirements.txt

# 3) Configure vari√°veis de ambiente (opcional)
cp .env.example .env  # edite se quiser

# 4) Coloque seus PDFs na pasta ./docs/
#    (ou aponte outra pasta com --docs / DOCS_DIR)

# 5) Ingest√£o
python ingest.py --docs ./docs

# 6) Consulta (exemplo)
python query.py --q "Como configuro pot√™ncia de leitura?" --k 5
```

## Build do chat e frontend

```bash
# Backend standalone
uvicorn app.main:app --reload  # roda em http://localhost:8000

# ou construa tudo com Docker
docker compose up --build

# Frontend (opcional para alterar a interface)
cd frontend
npm install
npm run build  # gera os arquivos em app/static
```

## Vari√°veis de ambiente
Copie `.env.example` para `.env` e ajuste conforme necess√°rio:

- **PGHOST**, **PGPORT**, **PGDATABASE**, **PGUSER**, **PGPASSWORD** ‚Äì conex√£o com o Postgres/pgvector.
- **DOCS_DIR** ‚Äì pasta padr√£o para os PDFs.
- **OPENAI_API_KEY**, **OPENAI_MODEL**, **USE_LLM** ‚Äì integra√ß√µes com LLM (opcional).
- **TOP_K**, **MAX_CONTEXT_CHARS** ‚Äì ajustes de recupera√ß√£o de trechos.
- **UPLOAD_DIR**, **UPLOAD_TTL**, **UPLOAD_MAX_SIZE**, **UPLOAD_ALLOWED_MIME_TYPES** ‚Äì controle de uploads tempor√°rios.
- **CORS_ALLOW_ORIGINS**, **BRAND_NAME**, **POWERED_BY_LABEL**, **LOGO_URL** ‚Äì personaliza√ß√£o da UI.

## Uso do chat
1. Garanta que o backend esteja rodando (com `uvicorn` ou Docker).
2. Acesse `http://localhost:8000` no navegador.
3. Envie mensagens pelo campo de texto. Opcionalmente, anexe um PDF pequeno para enriquecer o contexto.
4. Durante a gera√ß√£o da resposta, use **Cancelar** para interromper o streaming e **Enviar** novamente para retomar.

## Estrutura
```
pdf_knowledge_kit/
‚îú‚îÄ docker-compose.yml      # Postgres + pgvector
‚îú‚îÄ requirements.txt        # Depend√™ncias
‚îú‚îÄ schema.sql              # Cria√ß√£o de tabelas/√≠ndices
‚îú‚îÄ ingest.py               # Varre PDFs, extrai, fatia e insere
‚îú‚îÄ query.py                # Busca sem√¢ntica
‚îú‚îÄ .env.example            # Configs de conex√£o
‚îî‚îÄ docs/                   # Coloque seus PDFs aqui
```

## Integra√ß√£o no seu agente de IA (resumo)
- Use `query.py` como refer√™ncia: gere embedding da pergunta e rode SQL:
  `SELECT ... ORDER BY embedding <-> :vec LIMIT :k`.
- Traga os trechos + metadados e alimente o *prompt* do agente (*RAG*).
- Para respostas fi√©is, **mostre as fontes** (caminho do PDF e p√°gina).

## Dicas francas
- PDFs escaneados (sem texto) exigem **OCR** (ex.: Tesseract). Este kit n√£o faz OCR por padr√£o ‚Äî adicione se precisar.
- Para lotes grandes (milhares de p√°ginas), rode ingest√£o em *batches* e crie o √≠ndice **depois**.
- Se j√° usa Postgres no seu stack, pgvector √© simples e barato. Se quiser um servi√ßo dedicado, olhe **Qdrant** ou **Weaviate**.
 
## Crit√©rios de acessibilidade e desempenho
- Texto alternativo e r√≥tulos ARIA para componentes interativos.
- Navega√ß√£o total por teclado e foco vis√≠vel.
- Contraste m√≠nimo de 4.5:1 nas cores da interface.
- Respostas transmitidas via **SSE** para reduzir lat√™ncia.
- Limpeza autom√°tica de uploads e limites de tamanho para preservar recursos.

Boa constru√ß√£o! üöÄ
(gerado em 2025-08-18)
